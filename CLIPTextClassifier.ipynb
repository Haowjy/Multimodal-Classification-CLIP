{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with CLIP Text portion (https://github.com/openai/CLIP)\n",
    "\n",
    "uses masked self-attention Transformer as a text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/multinews/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe0851acb90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", jit=True, device=device)\n",
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakedditDataset(Dataset):\n",
    "    \"\"\"Subset of fake news dataset from \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, root_dir, image_preprocess=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (string): Path to the csv file or a pandas DF\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        if type(dataset) is str:\n",
    "            self.dataset = pd.read_csv(dataset)\n",
    "        else:\n",
    "            self.dataset = dataset\n",
    "        self.root_dir = root_dir\n",
    "        self.image_preprocess = image_preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        text = self.dataset.iloc[idx, 0]\n",
    "        img_name = os.path.join(self.root_dir, f\"{self.dataset.iloc[idx, 1]}.jpg\")\n",
    "        image = Image.open(img_name)\n",
    "        if self.image_preprocess:\n",
    "            image = self.image_preprocess(image.convert(\"RGB\"))\n",
    "            \n",
    "        label = torch.zeros(6)\n",
    "        label[self.dataset.iloc[idx, 2]] = 1\n",
    "        \n",
    "        return image, text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "trainset = FakedditDataset('train_clean.csv', 'data', image_preprocess=preprocess)\n",
    "testset = FakedditDataset('test_clean.csv', 'data', image_preprocess=preprocess)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) 36 torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, texts, labels = next(dataiter)\n",
    "print(images[0].shape, len(texts[0]), labels[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, device='cpu') -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.clip_layer, _ = clip.load(\"ViT-B/32\", jit=True, device=device) # Changed JIT to True for just inference\n",
    "        # output of clip is 512\n",
    "        # cat image and text for 1024\n",
    "        self.fc1 = nn.Linear(512, 512, device=device)\n",
    "        self.fc2 = nn.Linear(512, 128, device=device)\n",
    "        self.fc3 = nn.Linear(128, 6, device=device)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, text): # remove the image portion\n",
    "        text_features = self.clip_layer.encode_text(text).float()\n",
    "\n",
    "        x = self.relu(self.fc1(text_features))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# classifier = CLIPClassifier(device=device)\n",
    "# classifier.device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds and y are one-hot encoded\n",
    "def binary_accuracy(preds, y):\n",
    "    preds_label = torch.argmax(preds, dim=1)\n",
    "    y_label = torch.argmax(y, dim=1)\n",
    "    \n",
    "    correct = torch.sum(preds_label==y_label).item()\n",
    "    acc = correct / len(y_label)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/multinews/lib/python3.9/site-packages/clip/clip.py:159: FutureWarning: 'torch.onnx._patch_torch._node_getitem' is deprecated in version 1.13 and will be removed in version 1.14. Please Internally use '_node_get' in symbolic_helper instead..\n",
      "  if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\"cuda\"):\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "# define the initial learning rate here\n",
    "learning_rate = 1e-4\n",
    "n_epochs = 50 # how many epochs to run\n",
    "momentum = 0.9\n",
    "patience = 10 # number of times to observe worsening val set error before giving up\n",
    "MODEL_LOCATION = 'models/cliptextclassifier/'\n",
    "MODEL_VERSION = '1'\n",
    "MODEL_NAME = 'cliptextclassifier.pth'\n",
    "FULL_LOCATION = os.path.join(MODEL_LOCATION, MODEL_VERSION)\n",
    "MODEL_PATH = os.path.join(FULL_LOCATION, MODEL_NAME)\n",
    "os.makedirs(FULL_LOCATION, exist_ok=True)\n",
    "\n",
    "# define loss function and model\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = CLIPClassifier(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate, momentum=momentum)\n",
    "\n",
    "trainval_lossacc = {'train_loss':[], 'train_acc':[],'valid_loss':[],'valid_acc':[]}\n",
    "\n",
    "min_val_loss = math.inf\n",
    "epoch_no_improv = 0\n",
    "\n",
    "cur_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load from checkpoint if it exists\n",
    "# checkpoint = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# cur_epoch = checkpoint['epoch']\n",
    "# trainval_lossacc = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/multinews/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: operator() profile_node %594 : int[] = prim::profile_ivalue(%592)\n",
      " does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Step   200] loss: 0.671\n",
      "[Epoch 1, Step   400] loss: 0.636\n",
      "[Epoch 1, Step   600] loss: 0.572\n",
      "[1] \tTrain Loss: 0.602 | Train Acc: 31.28%\n",
      "[1] \t Val. Loss: 0.475 |  Val. Acc: 39.52%\n",
      "Min val loss 0.475\n",
      "\n",
      "[Epoch 2, Step   200] loss: 0.447\n",
      "[Epoch 2, Step   400] loss: 0.410\n",
      "[Epoch 2, Step   600] loss: 0.392\n",
      "[2] \tTrain Loss: 0.410 | Train Acc: 39.62%\n",
      "[2] \t Val. Loss: 0.381 |  Val. Acc: 39.51%\n",
      "Min val loss 0.381\n",
      "\n",
      "[Epoch 3, Step   200] loss: 0.383\n",
      "[Epoch 3, Step   400] loss: 0.383\n",
      "[Epoch 3, Step   600] loss: 0.380\n",
      "[3] \tTrain Loss: 0.382 | Train Acc: 39.52%\n",
      "[3] \t Val. Loss: 0.375 |  Val. Acc: 39.52%\n",
      "Min val loss 0.375\n",
      "\n",
      "[Epoch 4, Step   200] loss: 0.379\n",
      "[Epoch 4, Step   400] loss: 0.380\n",
      "[Epoch 4, Step   600] loss: 0.381\n",
      "[4] \tTrain Loss: 0.379 | Train Acc: 39.62%\n",
      "[4] \t Val. Loss: 0.374 |  Val. Acc: 39.51%\n",
      "Min val loss 0.374\n",
      "\n",
      "[Epoch 5, Step   200] loss: 0.381\n",
      "[Epoch 5, Step   400] loss: 0.379\n",
      "[Epoch 5, Step   600] loss: 0.377\n",
      "[5] \tTrain Loss: 0.378 | Train Acc: 39.61%\n",
      "[5] \t Val. Loss: 0.374 |  Val. Acc: 39.52%\n",
      "Min val loss 0.374\n",
      "\n",
      "[Epoch 6, Step   200] loss: 0.378\n",
      "[Epoch 6, Step   400] loss: 0.379\n",
      "[Epoch 6, Step   600] loss: 0.376\n",
      "[6] \tTrain Loss: 0.378 | Train Acc: 39.59%\n",
      "[6] \t Val. Loss: 0.374 |  Val. Acc: 39.52%\n",
      "Min val loss 0.374\n",
      "\n",
      "[Epoch 7, Step   200] loss: 0.377\n",
      "[Epoch 7, Step   400] loss: 0.379\n",
      "[Epoch 7, Step   600] loss: 0.376\n",
      "[7] \tTrain Loss: 0.377 | Train Acc: 39.70%\n",
      "[7] \t Val. Loss: 0.373 |  Val. Acc: 39.51%\n",
      "Min val loss 0.373\n",
      "\n",
      "[Epoch 8, Step   200] loss: 0.379\n",
      "[Epoch 8, Step   400] loss: 0.378\n",
      "[Epoch 8, Step   600] loss: 0.374\n",
      "[8] \tTrain Loss: 0.376 | Train Acc: 39.91%\n",
      "[8] \t Val. Loss: 0.373 |  Val. Acc: 39.53%\n",
      "Min val loss 0.373\n",
      "\n",
      "[Epoch 9, Step   200] loss: 0.377\n",
      "[Epoch 9, Step   400] loss: 0.378\n",
      "[Epoch 9, Step   600] loss: 0.374\n",
      "[9] \tTrain Loss: 0.376 | Train Acc: 40.05%\n",
      "[9] \t Val. Loss: 0.372 |  Val. Acc: 39.52%\n",
      "Min val loss 0.372\n",
      "\n",
      "[Epoch 10, Step   200] loss: 0.377\n",
      "[Epoch 10, Step   400] loss: 0.375\n",
      "[Epoch 10, Step   600] loss: 0.374\n",
      "[10] \tTrain Loss: 0.375 | Train Acc: 40.47%\n",
      "[10] \t Val. Loss: 0.371 |  Val. Acc: 39.52%\n",
      "Min val loss 0.371\n",
      "\n",
      "[Epoch 11, Step   200] loss: 0.374\n",
      "[Epoch 11, Step   400] loss: 0.377\n",
      "[Epoch 11, Step   600] loss: 0.371\n",
      "[11] \tTrain Loss: 0.374 | Train Acc: 41.60%\n",
      "[11] \t Val. Loss: 0.369 |  Val. Acc: 39.50%\n",
      "Min val loss 0.369\n",
      "\n",
      "[Epoch 12, Step   200] loss: 0.372\n",
      "[Epoch 12, Step   400] loss: 0.372\n",
      "[Epoch 12, Step   600] loss: 0.372\n",
      "[12] \tTrain Loss: 0.372 | Train Acc: 42.85%\n",
      "[12] \t Val. Loss: 0.367 |  Val. Acc: 39.50%\n",
      "Min val loss 0.367\n",
      "\n",
      "[Epoch 13, Step   200] loss: 0.369\n",
      "[Epoch 13, Step   400] loss: 0.368\n",
      "[Epoch 13, Step   600] loss: 0.371\n",
      "[13] \tTrain Loss: 0.369 | Train Acc: 44.94%\n",
      "[13] \t Val. Loss: 0.364 |  Val. Acc: 39.51%\n",
      "Min val loss 0.364\n",
      "\n",
      "[Epoch 14, Step   200] loss: 0.368\n",
      "[Epoch 14, Step   400] loss: 0.366\n",
      "[Epoch 14, Step   600] loss: 0.366\n",
      "[14] \tTrain Loss: 0.366 | Train Acc: 47.57%\n",
      "[14] \t Val. Loss: 0.360 |  Val. Acc: 54.04%\n",
      "Min val loss 0.360\n",
      "\n",
      "[Epoch 15, Step   200] loss: 0.363\n",
      "[Epoch 15, Step   400] loss: 0.363\n",
      "[Epoch 15, Step   600] loss: 0.361\n",
      "[15] \tTrain Loss: 0.362 | Train Acc: 51.56%\n",
      "[15] \t Val. Loss: 0.357 |  Val. Acc: 58.33%\n",
      "Min val loss 0.357\n",
      "\n",
      "[Epoch 16, Step   200] loss: 0.355\n",
      "[Epoch 16, Step   400] loss: 0.359\n",
      "[Epoch 16, Step   600] loss: 0.357\n",
      "[16] \tTrain Loss: 0.356 | Train Acc: 54.69%\n",
      "[16] \t Val. Loss: 0.350 |  Val. Acc: 60.07%\n",
      "Min val loss 0.350\n",
      "\n",
      "[Epoch 17, Step   200] loss: 0.353\n",
      "[Epoch 17, Step   400] loss: 0.350\n",
      "[Epoch 17, Step   600] loss: 0.349\n",
      "[17] \tTrain Loss: 0.349 | Train Acc: 58.08%\n",
      "[17] \t Val. Loss: 0.342 |  Val. Acc: 60.90%\n",
      "Min val loss 0.342\n",
      "\n",
      "[Epoch 18, Step   200] loss: 0.345\n",
      "[Epoch 18, Step   400] loss: 0.339\n",
      "[Epoch 18, Step   600] loss: 0.341\n",
      "[18] \tTrain Loss: 0.340 | Train Acc: 60.73%\n",
      "[18] \t Val. Loss: 0.332 |  Val. Acc: 61.71%\n",
      "Min val loss 0.332\n",
      "\n",
      "[Epoch 19, Step   200] loss: 0.332\n",
      "[Epoch 19, Step   400] loss: 0.332\n",
      "[Epoch 19, Step   600] loss: 0.327\n",
      "[19] \tTrain Loss: 0.329 | Train Acc: 61.85%\n",
      "[19] \t Val. Loss: 0.322 |  Val. Acc: 61.82%\n",
      "Min val loss 0.322\n",
      "\n",
      "[Epoch 20, Step   200] loss: 0.322\n",
      "[Epoch 20, Step   400] loss: 0.322\n",
      "[Epoch 20, Step   600] loss: 0.316\n",
      "[20] \tTrain Loss: 0.319 | Train Acc: 62.40%\n",
      "[20] \t Val. Loss: 0.313 |  Val. Acc: 62.21%\n",
      "Min val loss 0.313\n",
      "\n",
      "[Epoch 21, Step   200] loss: 0.314\n",
      "[Epoch 21, Step   400] loss: 0.307\n",
      "[Epoch 21, Step   600] loss: 0.307\n",
      "[21] \tTrain Loss: 0.308 | Train Acc: 63.03%\n",
      "[21] \t Val. Loss: 0.302 |  Val. Acc: 62.94%\n",
      "Min val loss 0.302\n",
      "\n",
      "[Epoch 22, Step   200] loss: 0.297\n",
      "[Epoch 22, Step   400] loss: 0.303\n",
      "[Epoch 22, Step   600] loss: 0.297\n",
      "[22] \tTrain Loss: 0.299 | Train Acc: 63.44%\n",
      "[22] \t Val. Loss: 0.298 |  Val. Acc: 62.41%\n",
      "Min val loss 0.298\n",
      "\n",
      "[Epoch 23, Step   200] loss: 0.296\n",
      "[Epoch 23, Step   400] loss: 0.287\n",
      "[Epoch 23, Step   600] loss: 0.288\n",
      "[23] \tTrain Loss: 0.291 | Train Acc: 63.90%\n",
      "[23] \t Val. Loss: 0.289 |  Val. Acc: 62.90%\n",
      "Min val loss 0.289\n",
      "\n",
      "[Epoch 24, Step   200] loss: 0.287\n",
      "[Epoch 24, Step   400] loss: 0.286\n",
      "[Epoch 24, Step   600] loss: 0.279\n",
      "[24] \tTrain Loss: 0.283 | Train Acc: 64.28%\n",
      "[24] \t Val. Loss: 0.285 |  Val. Acc: 63.13%\n",
      "Min val loss 0.285\n",
      "\n",
      "[Epoch 25, Step   200] loss: 0.279\n",
      "[Epoch 25, Step   400] loss: 0.278\n",
      "[Epoch 25, Step   600] loss: 0.275\n",
      "[25] \tTrain Loss: 0.277 | Train Acc: 64.56%\n",
      "[25] \t Val. Loss: 0.283 |  Val. Acc: 63.03%\n",
      "Min val loss 0.283\n",
      "\n",
      "[Epoch 26, Step   200] loss: 0.272\n",
      "[Epoch 26, Step   400] loss: 0.273\n",
      "[Epoch 26, Step   600] loss: 0.266\n",
      "[26] \tTrain Loss: 0.271 | Train Acc: 64.81%\n",
      "[26] \t Val. Loss: 0.279 |  Val. Acc: 62.81%\n",
      "Min val loss 0.279\n",
      "\n",
      "[Epoch 27, Step   200] loss: 0.273\n",
      "[Epoch 27, Step   400] loss: 0.259\n",
      "[Epoch 27, Step   600] loss: 0.265\n",
      "[27] \tTrain Loss: 0.264 | Train Acc: 65.20%\n",
      "[27] \t Val. Loss: 0.276 |  Val. Acc: 62.89%\n",
      "Min val loss 0.276\n",
      "\n",
      "[Epoch 28, Step   200] loss: 0.261\n",
      "[Epoch 28, Step   400] loss: 0.255\n",
      "[Epoch 28, Step   600] loss: 0.258\n",
      "[28] \tTrain Loss: 0.258 | Train Acc: 65.58%\n",
      "[28] \t Val. Loss: 0.274 |  Val. Acc: 63.33%\n",
      "Min val loss 0.274\n",
      "\n",
      "[Epoch 29, Step   200] loss: 0.249\n",
      "[Epoch 29, Step   400] loss: 0.257\n",
      "[Epoch 29, Step   600] loss: 0.246\n",
      "[29] \tTrain Loss: 0.251 | Train Acc: 66.17%\n",
      "[29] \t Val. Loss: 0.279 |  Val. Acc: 63.08%\n",
      "no improvement = 1\n",
      "\n",
      "[Epoch 30, Step   200] loss: 0.242\n",
      "[Epoch 30, Step   400] loss: 0.243\n",
      "[Epoch 30, Step   600] loss: 0.250\n",
      "[30] \tTrain Loss: 0.245 | Train Acc: 66.63%\n",
      "[30] \t Val. Loss: 0.271 |  Val. Acc: 63.30%\n",
      "Min val loss 0.271\n",
      "\n",
      "[Epoch 31, Step   200] loss: 0.238\n",
      "[Epoch 31, Step   400] loss: 0.239\n",
      "[Epoch 31, Step   600] loss: 0.233\n",
      "[31] \tTrain Loss: 0.236 | Train Acc: 67.63%\n",
      "[31] \t Val. Loss: 0.273 |  Val. Acc: 63.38%\n",
      "no improvement = 1\n",
      "\n",
      "[Epoch 32, Step   200] loss: 0.226\n",
      "[Epoch 32, Step   400] loss: 0.223\n",
      "[Epoch 32, Step   600] loss: 0.238\n",
      "[32] \tTrain Loss: 0.229 | Train Acc: 68.70%\n",
      "[32] \t Val. Loss: 0.286 |  Val. Acc: 62.89%\n",
      "no improvement = 2\n",
      "\n",
      "[Epoch 33, Step   200] loss: 0.219\n",
      "[Epoch 33, Step   400] loss: 0.220\n",
      "[Epoch 33, Step   600] loss: 0.228\n",
      "[33] \tTrain Loss: 0.223 | Train Acc: 70.35%\n",
      "[33] \t Val. Loss: 0.272 |  Val. Acc: 65.21%\n",
      "no improvement = 3\n",
      "\n",
      "[Epoch 34, Step   200] loss: 0.215\n",
      "[Epoch 34, Step   400] loss: 0.216\n",
      "[Epoch 34, Step   600] loss: 0.219\n",
      "[34] \tTrain Loss: 0.215 | Train Acc: 72.01%\n",
      "[34] \t Val. Loss: 0.277 |  Val. Acc: 64.07%\n",
      "no improvement = 4\n",
      "\n",
      "[Epoch 35, Step   200] loss: 0.208\n",
      "[Epoch 35, Step   400] loss: 0.204\n",
      "[Epoch 35, Step   600] loss: 0.208\n",
      "[35] \tTrain Loss: 0.208 | Train Acc: 73.80%\n",
      "[35] \t Val. Loss: 0.278 |  Val. Acc: 65.66%\n",
      "no improvement = 5\n",
      "\n",
      "[Epoch 36, Step   200] loss: 0.197\n",
      "[Epoch 36, Step   400] loss: 0.203\n",
      "[Epoch 36, Step   600] loss: 0.199\n",
      "[36] \tTrain Loss: 0.201 | Train Acc: 75.64%\n",
      "[36] \t Val. Loss: 0.282 |  Val. Acc: 65.57%\n",
      "no improvement = 6\n",
      "\n",
      "[Epoch 37, Step   200] loss: 0.195\n",
      "[Epoch 37, Step   400] loss: 0.188\n",
      "[Epoch 37, Step   600] loss: 0.195\n",
      "[37] \tTrain Loss: 0.193 | Train Acc: 77.25%\n",
      "[37] \t Val. Loss: 0.296 |  Val. Acc: 65.14%\n",
      "no improvement = 7\n",
      "\n",
      "[Epoch 38, Step   200] loss: 0.182\n",
      "[Epoch 38, Step   400] loss: 0.189\n",
      "[Epoch 38, Step   600] loss: 0.189\n",
      "[38] \tTrain Loss: 0.186 | Train Acc: 78.70%\n",
      "[38] \t Val. Loss: 0.284 |  Val. Acc: 65.65%\n",
      "no improvement = 8\n",
      "\n",
      "[Epoch 39, Step   200] loss: 0.186\n",
      "[Epoch 39, Step   400] loss: 0.179\n",
      "[Epoch 39, Step   600] loss: 0.183\n",
      "[39] \tTrain Loss: 0.182 | Train Acc: 79.58%\n",
      "[39] \t Val. Loss: 0.288 |  Val. Acc: 65.90%\n",
      "no improvement = 9\n",
      "\n",
      "[Epoch 40, Step   200] loss: 0.172\n",
      "[Epoch 40, Step   400] loss: 0.173\n",
      "[Epoch 40, Step   600] loss: 0.175\n",
      "[40] \tTrain Loss: 0.175 | Train Acc: 80.67%\n",
      "[40] \t Val. Loss: 0.292 |  Val. Acc: 65.27%\n",
      "Early Stopping\n",
      "no improvement = 10\n",
      "\n",
      "[Epoch 41, Step   200] loss: 0.165\n",
      "[Epoch 41, Step   400] loss: 0.168\n",
      "[Epoch 41, Step   600] loss: 0.169\n",
      "[41] \tTrain Loss: 0.168 | Train Acc: 81.94%\n",
      "[41] \t Val. Loss: 0.285 |  Val. Acc: 64.78%\n",
      "Early Stopping\n",
      "no improvement = 11\n",
      "\n",
      "[Epoch 42, Step   200] loss: 0.166\n",
      "[Epoch 42, Step   400] loss: 0.160\n",
      "[Epoch 42, Step   600] loss: 0.169\n",
      "[42] \tTrain Loss: 0.163 | Train Acc: 82.23%\n",
      "[42] \t Val. Loss: 0.291 |  Val. Acc: 65.12%\n",
      "Early Stopping\n",
      "no improvement = 12\n",
      "\n",
      "[Epoch 43, Step   200] loss: 0.157\n",
      "[Epoch 43, Step   400] loss: 0.154\n",
      "[Epoch 43, Step   600] loss: 0.160\n",
      "[43] \tTrain Loss: 0.158 | Train Acc: 82.77%\n",
      "[43] \t Val. Loss: 0.295 |  Val. Acc: 65.33%\n",
      "Early Stopping\n",
      "no improvement = 13\n",
      "\n",
      "[Epoch 44, Step   200] loss: 0.157\n",
      "[Epoch 44, Step   400] loss: 0.156\n",
      "[Epoch 44, Step   600] loss: 0.153\n",
      "[44] \tTrain Loss: 0.156 | Train Acc: 82.96%\n",
      "[44] \t Val. Loss: 0.299 |  Val. Acc: 66.17%\n",
      "Early Stopping\n",
      "no improvement = 14\n",
      "\n",
      "[Epoch 45, Step   200] loss: 0.152\n",
      "[Epoch 45, Step   400] loss: 0.155\n",
      "[Epoch 45, Step   600] loss: 0.146\n",
      "[45] \tTrain Loss: 0.153 | Train Acc: 82.98%\n",
      "[45] \t Val. Loss: 0.305 |  Val. Acc: 66.27%\n",
      "Early Stopping\n",
      "no improvement = 15\n",
      "\n",
      "[Epoch 46, Step   200] loss: 0.152\n",
      "[Epoch 46, Step   400] loss: 0.140\n",
      "[Epoch 46, Step   600] loss: 0.148\n",
      "[46] \tTrain Loss: 0.147 | Train Acc: 83.54%\n",
      "[46] \t Val. Loss: 0.305 |  Val. Acc: 65.22%\n",
      "Early Stopping\n",
      "no improvement = 16\n",
      "\n",
      "[Epoch 47, Step   200] loss: 0.144\n",
      "[Epoch 47, Step   400] loss: 0.142\n",
      "[Epoch 47, Step   600] loss: 0.141\n",
      "[47] \tTrain Loss: 0.142 | Train Acc: 83.89%\n",
      "[47] \t Val. Loss: 0.304 |  Val. Acc: 64.43%\n",
      "Early Stopping\n",
      "no improvement = 17\n",
      "\n",
      "[Epoch 48, Step   200] loss: 0.137\n",
      "[Epoch 48, Step   400] loss: 0.137\n",
      "[Epoch 48, Step   600] loss: 0.132\n",
      "[48] \tTrain Loss: 0.135 | Train Acc: 84.76%\n",
      "[48] \t Val. Loss: 0.313 |  Val. Acc: 66.00%\n",
      "Early Stopping\n",
      "no improvement = 18\n",
      "\n",
      "[Epoch 49, Step   200] loss: 0.135\n",
      "[Epoch 49, Step   400] loss: 0.137\n",
      "[Epoch 49, Step   600] loss: 0.135\n",
      "[49] \tTrain Loss: 0.137 | Train Acc: 84.24%\n",
      "[49] \t Val. Loss: 0.311 |  Val. Acc: 65.03%\n",
      "Early Stopping\n",
      "no improvement = 19\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(cur_epoch,n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # Training\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        _, texts, labels = data\n",
    "        \n",
    "        text_tokens = clip.tokenize(texts, truncate=True).to(device) # truncate: some titles are longer than 77, but I think there is more than enough context in 77 words\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        # zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        output = model(text_tokens)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += binary_accuracy(output, labels)\n",
    "        if i % 200 == 199:  # print every 200 mini-batches\n",
    "            print('[Epoch %d, Step %5d] loss: %.3f' %\n",
    "                  (epoch, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "    train_loss, train_acc = epoch_loss / len(trainloader), epoch_acc / len(trainloader)\n",
    "\n",
    "    trainval_lossacc['train_loss'].append(train_loss)\n",
    "    trainval_lossacc['train_acc'].append(train_acc)\n",
    "\n",
    "    # Evaluate with Test dataset\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            _, texts, labels = data\n",
    "            text_tokens = clip.tokenize(texts, truncate=True).to(device) # truncate: some titles are longer than 77, but I think there is more than enough context in 77 words\n",
    "            # print(text_tokens.shape, text_tokens.dtype)\n",
    "            labels = labels.float().to(device)\n",
    "            # Forward \n",
    "            output = model(text_tokens)\n",
    "            \n",
    "            # Compute the loss using the final output\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += binary_accuracy(output, labels)\n",
    "    \n",
    "    valid_loss, valid_acc = epoch_loss / len(testloader), epoch_acc / len(testloader)\n",
    "\n",
    "    trainval_lossacc['valid_loss'].append(valid_loss)\n",
    "    trainval_lossacc['valid_acc'].append(valid_acc)\n",
    "    # Showing statistics\n",
    "    print(f'[{epoch}] \\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'[{epoch}] \\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    # Early stopping condition\n",
    "    # https://stackoverflow.com/questions/60200088/how-to-make-early-stopping-in-image-classification-pytorch\n",
    "    if valid_loss < min_val_loss:\n",
    "        min_val_loss = valid_loss\n",
    "        epoch_no_improv = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': trainval_lossacc,\n",
    "            }, MODEL_PATH)\n",
    "        \n",
    "        print(f'Min val loss {min_val_loss:.3f}')\n",
    "    else:\n",
    "        epoch_no_improv += 1\n",
    "        if epoch_no_improv >= patience:\n",
    "            print('Early Stopping')\n",
    "            break\n",
    "            # os.makedirs(f'drive/Shareddrives/MultimodalNews/models/{MODEL_VERSION}/', exist_ok=True)\n",
    "            # torch.save(best_model, f\"drive/Shareddrives/MultimodalNews/models/{MODEL_VERSION}/clipclassifier.pth\")\n",
    "        print(f\"no improvement = {epoch_no_improv}\")\n",
    "    print()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from checkpoint if it exists\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "cur_epoch = checkpoint['epoch']\n",
    "trainval_lossacc = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train loss: 0.24503\n",
      "final train acc: 0.66633\n",
      "\n",
      "final valid loss: 0.27108\n",
      "final valid acc: 0.63299\n"
     ]
    }
   ],
   "source": [
    "print(f\"final train loss: {trainval_lossacc['train_loss'][-1]:.5f}\")\n",
    "print(f\"final train acc: {trainval_lossacc['train_acc'][-1]:.5f}\")\n",
    "print()\n",
    "print(f\"final valid loss: {trainval_lossacc['valid_loss'][-1]:.5f}\")\n",
    "print(f\"final valid acc: {trainval_lossacc['valid_acc'][-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30] \tTrain Loss: 0.24503 | Train Acc: 66.63274%\n",
      "[30] \t Val. Loss: 0.27108 |  Val. Acc: 63.29861%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{cur_epoch}] \\tTrain Loss: {trainval_lossacc['train_loss'][-1]:.5f} | Train Acc: {trainval_lossacc['train_acc'][-1]*100:.5f}%\")\n",
    "print(f\"[{cur_epoch}] \\t Val. Loss: {trainval_lossacc['valid_loss'][-1]:.5f} |  Val. Acc: {trainval_lossacc['valid_acc'][-1]*100:.5f}%\")\n",
    "# [30] \tTrain Loss: 0.24503 | Train Acc: 66.63274%\n",
    "# [30] \t Val. Loss: 0.27108 |  Val. Acc: 63.29861%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multinews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12a1584764fc80bec723ae766c7274bc1045a0de66e94735d00d14d294f6d446"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
