{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with CLIP by only text or image through linear probing (https://github.com/openai/CLIP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/multinews/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f05f009cb50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/multinews/lib/python3.9/site-packages/clip/clip.py:159: FutureWarning: 'torch.onnx._patch_torch._node_getitem' is deprecated in version 1.13 and will be removed in version 1.14. Please Internally use '_node_get' in symbolic_helper instead..\n",
      "  if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\"cuda\"):\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", jit=True, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakedditDataset(Dataset):\n",
    "    \"\"\"Subset of fake news dataset from \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, root_dir, image_preprocess=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (string): Path to the csv file or a pandas DF\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        if type(dataset) is str:\n",
    "            self.dataset = pd.read_csv(dataset)\n",
    "        else:\n",
    "            self.dataset = dataset\n",
    "        self.root_dir = root_dir\n",
    "        self.image_preprocess = image_preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        text = self.dataset.iloc[idx, 0]\n",
    "        img_name = os.path.join(self.root_dir, f\"{self.dataset.iloc[idx, 1]}.jpg\")\n",
    "        image = Image.open(img_name)\n",
    "        if self.image_preprocess:\n",
    "            image = self.image_preprocess(image.convert(\"RGB\"))\n",
    "            \n",
    "        label = torch.zeros(6)\n",
    "        label[self.dataset.iloc[idx, 2]] = 1\n",
    "        \n",
    "        return image, text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "trainset = FakedditDataset('train_clean.csv', 'data', image_preprocess=preprocess)\n",
    "testset = FakedditDataset('test_clean.csv', 'data', image_preprocess=preprocess)\n",
    "\n",
    "# trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "# testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Logistic Regression)\n",
    "based on code from <https://github.com/openai/CLIP>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [03:07<00:00,  1.28it/s]\n",
      "100%|██████████| 80/80 [01:03<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_image_features = []\n",
    "    all_text_features = []\n",
    "    all_concat_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, texts, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            image_input = torch.tensor(np.stack(images)).to(device)\n",
    "            text_tokens = clip.tokenize(texts, truncate=True).to(device) # truncate: some titles are longer than 77, but I think there is more than enough context in 77 words\n",
    "            labels = labels.float().to(device)\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "            \n",
    "            image_features = model.encode_image(image_input)\n",
    "            all_image_features.append(image_features)\n",
    "            \n",
    "            text_features = model.encode_text(text_tokens)\n",
    "            all_text_features.append(text_features)\n",
    "            \n",
    "            all_concat_features.append(torch.cat([image_features, text_features], dim=1))\n",
    "            \n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_image_features).cpu().numpy(), torch.cat(all_text_features).cpu().numpy(), torch.cat(all_concat_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "train_image_features, train_text_features, train_concat_features, train_labels = get_features(trainset)\n",
    "test_image_features, test_text_features, test_concat_features, test_labels = get_features(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Linear Probing with Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3078     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.29700D+04    |proj g|=  3.38006D+04\n",
      "\n",
      "At iterate   50    f=  1.19017D+04    |proj g|=  7.50322D+02\n",
      "\n",
      "At iterate  100    f=  1.16059D+04    |proj g|=  1.34348D+02\n",
      "\n",
      "At iterate  150    f=  1.15707D+04    |proj g|=  3.71698D+01\n",
      "\n",
      "At iterate  200    f=  1.15637D+04    |proj g|=  1.76459D+01\n",
      "\n",
      "At iterate  250    f=  1.15608D+04    |proj g|=  2.15426D+01\n",
      "\n",
      "At iterate  300    f=  1.15586D+04    |proj g|=  1.73510D+01\n",
      "\n",
      "At iterate  350    f=  1.15569D+04    |proj g|=  8.60436D+00\n",
      "\n",
      "At iterate  400    f=  1.15553D+04    |proj g|=  3.24305D+01\n",
      "\n",
      "At iterate  450    f=  1.15545D+04    |proj g|=  7.38039D+00\n",
      "\n",
      "At iterate  500    f=  1.15541D+04    |proj g|=  4.57593D+00\n",
      "\n",
      "At iterate  550    f=  1.15539D+04    |proj g|=  3.40096D+00\n",
      "\n",
      "At iterate  600    f=  1.15539D+04    |proj g|=  2.29648D+00\n",
      "\n",
      "At iterate  650    f=  1.15539D+04    |proj g|=  1.42990D+00\n",
      "\n",
      "At iterate  700    f=  1.15539D+04    |proj g|=  1.42042D+00\n",
      "\n",
      "At iterate  750    f=  1.15539D+04    |proj g|=  9.12951D-01\n",
      "\n",
      "At iterate  800    f=  1.15539D+04    |proj g|=  6.15463D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3078    818    913      1     0     0   1.734D+00   1.155D+04\n",
      "  F =   11553.867667609447     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   25.7s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.316, max_iter=1000, random_state=42, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.316, max_iter=1000, random_state=42, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.316, max_iter=1000, random_state=42, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_classifier = LogisticRegression(random_state=42, C=0.316, max_iter=1000, verbose=1)\n",
    "image_classifier.fit(train_image_features, train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Linear Probing with Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3078     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.29700D+04    |proj g|=  2.53215D+04\n",
      "\n",
      "At iterate   50    f=  1.55261D+04    |proj g|=  1.44825D+03\n",
      "\n",
      "At iterate  100    f=  1.49454D+04    |proj g|=  1.85337D+02\n",
      "\n",
      "At iterate  150    f=  1.48468D+04    |proj g|=  1.04991D+02\n",
      "\n",
      "At iterate  200    f=  1.48219D+04    |proj g|=  3.70467D+01\n",
      "\n",
      "At iterate  250    f=  1.48124D+04    |proj g|=  4.84347D+01\n",
      "\n",
      "At iterate  300    f=  1.48088D+04    |proj g|=  7.50601D+00\n",
      "\n",
      "At iterate  350    f=  1.48071D+04    |proj g|=  1.22280D+01\n",
      "\n",
      "At iterate  400    f=  1.48058D+04    |proj g|=  1.81465D+01\n",
      "\n",
      "At iterate  450    f=  1.48052D+04    |proj g|=  2.50352D+00\n",
      "\n",
      "At iterate  500    f=  1.48048D+04    |proj g|=  6.19947D+00\n",
      "\n",
      "At iterate  550    f=  1.48046D+04    |proj g|=  1.18652D+01\n",
      "\n",
      "At iterate  600    f=  1.48045D+04    |proj g|=  8.95061D+00\n",
      "\n",
      "At iterate  650    f=  1.48045D+04    |proj g|=  5.85791D-01\n",
      "\n",
      "At iterate  700    f=  1.48044D+04    |proj g|=  1.52226D+00\n",
      "\n",
      "At iterate  750    f=  1.48044D+04    |proj g|=  9.98562D-01\n",
      "\n",
      "At iterate  800    f=  1.48044D+04    |proj g|=  9.71790D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3078    822    897      1     0     0   3.705D-01   1.480D+04\n",
      "  F =   14804.374190986553     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   40.3s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.316, max_iter=1000, random_state=42, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.316, max_iter=1000, random_state=42, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.316, max_iter=1000, random_state=42, verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier = LogisticRegression(random_state=42, C=0.316, max_iter=1000, verbose=1)\n",
    "text_classifier.fit(train_text_features, train_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Linear Probing with Both Image and Text (Concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         6150     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.29700D+04    |proj g|=  3.38006D+04\n",
      "\n",
      "At iterate   50    f=  7.28675D+03    |proj g|=  1.31749D+02\n",
      "\n",
      "At iterate  100    f=  6.41206D+03    |proj g|=  1.24925D+02\n",
      "\n",
      "At iterate  150    f=  6.26398D+03    |proj g|=  3.31211D+02\n",
      "\n",
      "At iterate  200    f=  6.22455D+03    |proj g|=  4.44137D+01\n",
      "\n",
      "At iterate  250    f=  6.21265D+03    |proj g|=  5.19687D+01\n",
      "\n",
      "At iterate  300    f=  6.20928D+03    |proj g|=  5.53868D+00\n",
      "\n",
      "At iterate  350    f=  6.20811D+03    |proj g|=  8.03352D+00\n",
      "\n",
      "At iterate  400    f=  6.20755D+03    |proj g|=  3.39356D+00\n",
      "\n",
      "At iterate  450    f=  6.20689D+03    |proj g|=  1.89719D+01\n",
      "\n",
      "At iterate  500    f=  6.20610D+03    |proj g|=  7.94628D+00\n",
      "\n",
      "At iterate  550    f=  6.20535D+03    |proj g|=  4.20252D+00\n",
      "\n",
      "At iterate  600    f=  6.20457D+03    |proj g|=  6.29080D+00\n",
      "\n",
      "At iterate  650    f=  6.20412D+03    |proj g|=  7.90683D+00\n",
      "\n",
      "At iterate  700    f=  6.20392D+03    |proj g|=  3.97669D+00\n",
      "\n",
      "At iterate  750    f=  6.20382D+03    |proj g|=  6.91818D-01\n",
      "\n",
      "At iterate  800    f=  6.20379D+03    |proj g|=  7.42180D-01\n",
      "\n",
      "At iterate  850    f=  6.20377D+03    |proj g|=  7.18327D-01\n",
      "\n",
      "At iterate  900    f=  6.20376D+03    |proj g|=  4.89657D+00\n",
      "\n",
      "At iterate  950    f=  6.20374D+03    |proj g|=  9.22222D-01\n",
      "\n",
      "At iterate 1000    f=  6.20373D+03    |proj g|=  1.18210D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 6150   1000   1089      1     0     0   1.182D+00   6.204D+03\n",
      "  F =   6203.7292959677843     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/multinews/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.316, max_iter=1000, random_state=42, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.316, max_iter=1000, random_state=42, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.316, max_iter=1000, random_state=42, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_classifier = LogisticRegression(random_state=42, C=0.316, max_iter=1000, verbose=1)\n",
    "concat_classifier.fit(train_concat_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "os.makedirs('models/linearimage/1', exist_ok=True)\n",
    "\n",
    "with open('models/linearimage/1/image_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(image_classifier, f)\n",
    "\n",
    "os.makedirs('models/lineartext/1', exist_ok=True)\n",
    "\n",
    "with open('models/lineartext/1/text_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(text_classifier, f)\n",
    "\n",
    "os.makedirs('models/linearconcat/1', exist_ok=True)\n",
    "\n",
    "with open('models/linearconcat/1/concat_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(concat_classifier, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('models/linearimage/1/image_classifier.pkl', 'rb') as f:\n",
    "#     image_classifier = pickle.load(f)\n",
    "\n",
    "# with open('models/lineartext/1/text_classifier.pkl', 'rb') as f:\n",
    "#     text_classifier = pickle.load(f)\n",
    "    \n",
    "# with open('models/linearconcat/1/concat_classifier.pkl', 'rb') as f:\n",
    "#     concat_classifier = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(model, features, labels):\n",
    "    preds = model.predict(features)\n",
    "    return np.mean((labels == preds).astype(np.float64)) * 100.\n",
    "\n",
    "def eval_score(model, features, labels):\n",
    "    return model.score(features, labels) * 100.\n",
    "    # return np.mean((labels == np.argmax(preds, axis=1)).astype(np.float64)) * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 82.775%\n",
      "Test Accuracy = 79.787%\n"
     ]
    }
   ],
   "source": [
    "train_image_acc = eval_score(image_classifier, train_image_features, train_labels)\n",
    "test_image_acc = eval_score(image_classifier, test_image_features, test_labels)\n",
    "\n",
    "# preds = image_classifier.predict(train_image_features)\n",
    "# accuracy = np.mean((train_labels == preds).astype(np.float64)) * 100.\n",
    "print(f\"Train Accuracy = {train_image_acc:.3f}%\")\n",
    "\n",
    "# preds = image_classifier.predict(train_image_features)\n",
    "# accuracy = np.mean((test_labels == preds).astype(np.float64)) * 100.\n",
    "print(f\"Test Accuracy = {test_image_acc:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 77.892%\n",
      "Test Accuracy = 74.459%\n"
     ]
    }
   ],
   "source": [
    "train_text_acc = eval_acc(text_classifier, train_text_features, train_labels)\n",
    "test_text_acc = eval_acc(text_classifier, test_text_features, test_labels)\n",
    "\n",
    "# preds = text_classifier.predict(train_text_features)\n",
    "# accuracy = np.mean((train_labels == preds).astype(np.float64)) * 100.\n",
    "print(f\"Train Accuracy = {train_text_acc:.3f}%\")\n",
    "\n",
    "# preds = text_classifier.predict(test_text_features)\n",
    "# accuracy = np.mean((test_labels == preds).astype(np.float64)) * 100.\n",
    "print(f\"Test Accuracy = {test_text_acc:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 91.702%\n",
      "Test Accuracy = 86.029%\n"
     ]
    }
   ],
   "source": [
    "train_concat_acc = eval_acc(concat_classifier, train_concat_features, train_labels)\n",
    "test_concat_acc = eval_acc(concat_classifier, test_concat_features, test_labels)\n",
    "\n",
    "# preds = concat_classifier.predict(train_concat_features)\n",
    "# accuracy = np.mean((train_labels == preds).astype(np.float64)) * 100.\n",
    "print(f\"Train Accuracy = {train_concat_acc:.3f}%\")\n",
    "\n",
    "# preds = concat_classifier.predict(test_concat_features)\n",
    "# accuracy = np.mean((test_labels == preds).astype(np.float64)) * 100.\n",
    "print(f\"Test Accuracy = {test_concat_acc:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_724/3682273022.py:3: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  df.to_latex('linearprobe-results.tex', index=False)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'model': ['concat', 'text', 'image'], 'Train Accuracy': [f\"{train_concat_acc:.3f}%\", f\"{train_text_acc:.3f}%\", f\"{train_image_acc:.3f}%\"], 'Test Accuracy': [f\"{test_concat_acc:.3f}%\", f\"{test_text_acc:.3f}%\", f\"{test_image_acc:.3f}%\"]})\n",
    "\n",
    "df.to_latex('linearprobe-results.tex', index=False)\n",
    "# \\begin{tabular}{lll}\n",
    "# \\toprule\n",
    "#  model & Train Accuracy & Test Accuracy \\\\\n",
    "# \\midrule\n",
    "# concat &        91.702\\% &       86.029\\% \\\\\n",
    "#   text &        77.892\\% &       74.459\\% \\\\\n",
    "#  image &        82.775\\% &       79.787\\% \\\\\n",
    "# \\bottomrule\n",
    "# \\end{tabular}\n",
    "\n",
    "\n",
    "# CLIP\n",
    "# [36] \tTrain Loss: 0.14999 | Train Acc: 82.17381%\n",
    "# [36] \t Val. Loss: 0.21184 |  Val. Acc: 72.90648%\n",
    "# CLIP Text\n",
    "# [30] \tTrain Loss: 0.24503 | Train Acc: 66.63274%\n",
    "# [30] \t Val. Loss: 0.27108 |  Val. Acc: 63.29861%\n",
    "# CLIP Image\n",
    "# [25] \tTrain Loss: 0.24470 | Train Acc: 66.43988%\n",
    "# [25] \t Val. Loss: 0.26212 |  Val. Acc: 64.21204%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multinews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12a1584764fc80bec723ae766c7274bc1045a0de66e94735d00d14d294f6d446"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
