{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with CLIP Image Portion (https://github.com/openai/CLIP)\n",
    "\n",
    "using ViT/32\n",
    "(vision transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/multinews/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f078c070b90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/multinews/lib/python3.9/site-packages/clip/clip.py:159: FutureWarning: 'torch.onnx._patch_torch._node_getitem' is deprecated in version 1.13 and will be removed in version 1.14. Please Internally use '_node_get' in symbolic_helper instead..\n",
      "  if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\"cuda\"):\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", jit=True, device=device) # Must set jit=False for training\n",
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakedditDataset(Dataset):\n",
    "    \"\"\"Subset of fake news dataset from \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, root_dir, image_preprocess=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (string): Path to the csv file or a pandas DF\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        if type(dataset) is str:\n",
    "            self.dataset = pd.read_csv(dataset)\n",
    "        else:\n",
    "            self.dataset = dataset\n",
    "        self.root_dir = root_dir\n",
    "        self.image_preprocess = image_preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        text = self.dataset.iloc[idx, 0]\n",
    "        img_name = os.path.join(self.root_dir, f\"{self.dataset.iloc[idx, 1]}.jpg\")\n",
    "        image = Image.open(img_name)\n",
    "        if self.image_preprocess:\n",
    "            image = self.image_preprocess(image.convert(\"RGB\"))\n",
    "            \n",
    "        label = torch.zeros(6)\n",
    "        label[self.dataset.iloc[idx, 2]] = 1\n",
    "        \n",
    "        return image, text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "trainset = FakedditDataset('train_clean.csv', 'data', image_preprocess=preprocess)\n",
    "testset = FakedditDataset('test_clean.csv', 'data', image_preprocess=preprocess)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) 39 torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, texts, labels = next(dataiter)\n",
    "print(images[0].shape, len(texts[0]), labels[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, device='cpu') -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.clip_layer, _ = clip.load(\"ViT-B/32\", jit=True, device=device) # Changed JIT to True for just inference\n",
    "        # output of clip is 512\n",
    "        # cat image and text for 1024\n",
    "        self.fc1 = nn.Linear(512, 512, device=device)\n",
    "        self.fc2 = nn.Linear(512, 128, device=device)\n",
    "        self.fc3 = nn.Linear(128, 6, device=device)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        image_features = self.clip_layer.encode_image(image).float()\n",
    "\n",
    "        x = self.relu(self.fc1(image_features))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# classifier = CLIPClassifier(device=device)\n",
    "# classifier.device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds and y are one-hot encoded\n",
    "def binary_accuracy(preds, y):\n",
    "    preds_label = torch.argmax(preds, dim=1)\n",
    "    y_label = torch.argmax(y, dim=1)\n",
    "    \n",
    "    correct = torch.sum(preds_label==y_label).item()\n",
    "    acc = correct / len(y_label)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/multinews/lib/python3.9/site-packages/clip/clip.py:159: FutureWarning: 'torch.onnx._patch_torch._node_getitem' is deprecated in version 1.13 and will be removed in version 1.14. Please Internally use '_node_get' in symbolic_helper instead..\n",
      "  if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\"cuda\"):\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "# define the initial learning rate here\n",
    "learning_rate = 1e-4\n",
    "n_epochs = 50 # how many epochs to run\n",
    "momentum = 0.9\n",
    "patience = 10 # number of times to observe worsening val set error before giving up\n",
    "MODEL_LOCATION = 'models/'\n",
    "MODEL_VERSION = '4'\n",
    "FULL_LOCATION = os.path.join(MODEL_LOCATION, MODEL_VERSION)\n",
    "MODEL_PATH = os.path.join(FULL_LOCATION, f'clipimageclassifier.pth')\n",
    "os.makedirs(FULL_LOCATION, exist_ok=True)\n",
    "\n",
    "# define loss function and model\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = CLIPClassifier(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate, momentum=momentum)\n",
    "\n",
    "trainval_lossacc = {'train_loss':[], 'train_acc':[],'valid_loss':[],'valid_acc':[]}\n",
    "\n",
    "min_val_loss = math.inf\n",
    "epoch_no_improv = 0\n",
    "\n",
    "cur_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load from checkpoint if it exists\n",
    "# checkpoint = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# cur_epoch = checkpoint['epoch']\n",
    "# trainval_lossacc = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyyao/miniconda3/envs/multinews/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: operator() profile_node %594 : int[] = prim::profile_ivalue(%592)\n",
      " does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Step   200] loss: 0.677\n",
      "[Epoch 1, Step   400] loss: 0.649\n",
      "[Epoch 1, Step   600] loss: 0.596\n",
      "[1] \tTrain Loss: 0.618 | Train Acc: 27.84%\n",
      "[1] \t Val. Loss: 0.491 |  Val. Acc: 39.52%\n",
      "Min val loss 0.491\n",
      "\n",
      "[Epoch 2, Step   200] loss: 0.456\n",
      "[Epoch 2, Step   400] loss: 0.412\n",
      "[Epoch 2, Step   600] loss: 0.391\n",
      "[2] \tTrain Loss: 0.413 | Train Acc: 39.57%\n",
      "[2] \t Val. Loss: 0.380 |  Val. Acc: 39.50%\n",
      "Min val loss 0.380\n",
      "\n",
      "[Epoch 3, Step   200] loss: 0.381\n",
      "[Epoch 3, Step   400] loss: 0.380\n",
      "[Epoch 3, Step   600] loss: 0.379\n",
      "[3] \tTrain Loss: 0.380 | Train Acc: 39.62%\n",
      "[3] \t Val. Loss: 0.375 |  Val. Acc: 39.51%\n",
      "Min val loss 0.375\n",
      "\n",
      "[Epoch 4, Step   200] loss: 0.378\n",
      "[Epoch 4, Step   400] loss: 0.381\n",
      "[Epoch 4, Step   600] loss: 0.377\n",
      "[4] \tTrain Loss: 0.378 | Train Acc: 39.53%\n",
      "[4] \t Val. Loss: 0.374 |  Val. Acc: 39.51%\n",
      "Min val loss 0.374\n",
      "\n",
      "[Epoch 5, Step   200] loss: 0.382\n",
      "[Epoch 5, Step   400] loss: 0.375\n",
      "[Epoch 5, Step   600] loss: 0.376\n",
      "[5] \tTrain Loss: 0.377 | Train Acc: 39.48%\n",
      "[5] \t Val. Loss: 0.373 |  Val. Acc: 39.51%\n",
      "Min val loss 0.373\n",
      "\n",
      "[Epoch 6, Step   200] loss: 0.374\n",
      "[Epoch 6, Step   400] loss: 0.376\n",
      "[Epoch 6, Step   600] loss: 0.378\n",
      "[6] \tTrain Loss: 0.376 | Train Acc: 39.49%\n",
      "[6] \t Val. Loss: 0.372 |  Val. Acc: 39.51%\n",
      "Min val loss 0.372\n",
      "\n",
      "[Epoch 7, Step   200] loss: 0.374\n",
      "[Epoch 7, Step   400] loss: 0.374\n",
      "[Epoch 7, Step   600] loss: 0.375\n",
      "[7] \tTrain Loss: 0.374 | Train Acc: 39.80%\n",
      "[7] \t Val. Loss: 0.369 |  Val. Acc: 39.51%\n",
      "Min val loss 0.369\n",
      "\n",
      "[Epoch 8, Step   200] loss: 0.372\n",
      "[Epoch 8, Step   400] loss: 0.373\n",
      "[Epoch 8, Step   600] loss: 0.367\n",
      "[8] \tTrain Loss: 0.370 | Train Acc: 41.23%\n",
      "[8] \t Val. Loss: 0.365 |  Val. Acc: 39.51%\n",
      "Min val loss 0.365\n",
      "\n",
      "[Epoch 9, Step   200] loss: 0.366\n",
      "[Epoch 9, Step   400] loss: 0.366\n",
      "[Epoch 9, Step   600] loss: 0.363\n",
      "[9] \tTrain Loss: 0.364 | Train Acc: 46.88%\n",
      "[9] \t Val. Loss: 0.357 |  Val. Acc: 55.98%\n",
      "Min val loss 0.357\n",
      "\n",
      "[Epoch 10, Step   200] loss: 0.359\n",
      "[Epoch 10, Step   400] loss: 0.356\n",
      "[Epoch 10, Step   600] loss: 0.353\n",
      "[10] \tTrain Loss: 0.354 | Train Acc: 57.17%\n",
      "[10] \t Val. Loss: 0.345 |  Val. Acc: 61.30%\n",
      "Min val loss 0.345\n",
      "\n",
      "[Epoch 11, Step   200] loss: 0.344\n",
      "[Epoch 11, Step   400] loss: 0.341\n",
      "[Epoch 11, Step   600] loss: 0.339\n",
      "[11] \tTrain Loss: 0.340 | Train Acc: 61.67%\n",
      "[11] \t Val. Loss: 0.329 |  Val. Acc: 62.10%\n",
      "Min val loss 0.329\n",
      "\n",
      "[Epoch 12, Step   200] loss: 0.328\n",
      "[Epoch 12, Step   400] loss: 0.325\n",
      "[Epoch 12, Step   600] loss: 0.321\n",
      "[12] \tTrain Loss: 0.322 | Train Acc: 62.65%\n",
      "[12] \t Val. Loss: 0.309 |  Val. Acc: 63.42%\n",
      "Min val loss 0.309\n",
      "\n",
      "[Epoch 13, Step   200] loss: 0.312\n",
      "[Epoch 13, Step   400] loss: 0.309\n",
      "[Epoch 13, Step   600] loss: 0.304\n",
      "[13] \tTrain Loss: 0.308 | Train Acc: 62.73%\n",
      "[13] \t Val. Loss: 0.296 |  Val. Acc: 63.40%\n",
      "Min val loss 0.296\n",
      "\n",
      "[Epoch 14, Step   200] loss: 0.301\n",
      "[Epoch 14, Step   400] loss: 0.297\n",
      "[Epoch 14, Step   600] loss: 0.294\n",
      "[14] \tTrain Loss: 0.296 | Train Acc: 63.13%\n",
      "[14] \t Val. Loss: 0.288 |  Val. Acc: 63.79%\n",
      "Min val loss 0.288\n",
      "\n",
      "[Epoch 15, Step   200] loss: 0.288\n",
      "[Epoch 15, Step   400] loss: 0.288\n",
      "[Epoch 15, Step   600] loss: 0.288\n",
      "[15] \tTrain Loss: 0.288 | Train Acc: 63.50%\n",
      "[15] \t Val. Loss: 0.279 |  Val. Acc: 64.30%\n",
      "Min val loss 0.279\n",
      "\n",
      "[Epoch 16, Step   200] loss: 0.281\n",
      "[Epoch 16, Step   400] loss: 0.284\n",
      "[Epoch 16, Step   600] loss: 0.282\n",
      "[16] \tTrain Loss: 0.282 | Train Acc: 63.67%\n",
      "[16] \t Val. Loss: 0.293 |  Val. Acc: 60.74%\n",
      "no improvement = 1\n",
      "\n",
      "[Epoch 17, Step   200] loss: 0.275\n",
      "[Epoch 17, Step   400] loss: 0.280\n",
      "[Epoch 17, Step   600] loss: 0.274\n",
      "[17] \tTrain Loss: 0.275 | Train Acc: 64.10%\n",
      "[17] \t Val. Loss: 0.277 |  Val. Acc: 63.47%\n",
      "Min val loss 0.277\n",
      "\n",
      "[Epoch 18, Step   200] loss: 0.264\n",
      "[Epoch 18, Step   400] loss: 0.271\n",
      "[Epoch 18, Step   600] loss: 0.278\n",
      "[18] \tTrain Loss: 0.272 | Train Acc: 64.32%\n",
      "[18] \t Val. Loss: 0.276 |  Val. Acc: 63.04%\n",
      "Min val loss 0.276\n",
      "\n",
      "[Epoch 19, Step   200] loss: 0.267\n",
      "[Epoch 19, Step   400] loss: 0.268\n",
      "[Epoch 19, Step   600] loss: 0.268\n",
      "[19] \tTrain Loss: 0.268 | Train Acc: 64.75%\n",
      "[19] \t Val. Loss: 0.270 |  Val. Acc: 64.04%\n",
      "Min val loss 0.270\n",
      "\n",
      "[Epoch 20, Step   200] loss: 0.261\n",
      "[Epoch 20, Step   400] loss: 0.265\n",
      "[Epoch 20, Step   600] loss: 0.264\n",
      "[20] \tTrain Loss: 0.263 | Train Acc: 64.85%\n",
      "[20] \t Val. Loss: 0.268 |  Val. Acc: 64.00%\n",
      "Min val loss 0.268\n",
      "\n",
      "[Epoch 21, Step   200] loss: 0.255\n",
      "[Epoch 21, Step   400] loss: 0.258\n",
      "[Epoch 21, Step   600] loss: 0.258\n",
      "[21] \tTrain Loss: 0.259 | Train Acc: 65.44%\n",
      "[21] \t Val. Loss: 0.275 |  Val. Acc: 62.68%\n",
      "no improvement = 1\n",
      "\n",
      "[Epoch 22, Step   200] loss: 0.252\n",
      "[Epoch 22, Step   400] loss: 0.262\n",
      "[Epoch 22, Step   600] loss: 0.252\n",
      "[22] \tTrain Loss: 0.256 | Train Acc: 65.66%\n",
      "[22] \t Val. Loss: 0.283 |  Val. Acc: 62.79%\n",
      "no improvement = 2\n",
      "\n",
      "[Epoch 23, Step   200] loss: 0.252\n",
      "[Epoch 23, Step   400] loss: 0.248\n",
      "[Epoch 23, Step   600] loss: 0.252\n",
      "[23] \tTrain Loss: 0.253 | Train Acc: 65.85%\n",
      "[23] \t Val. Loss: 0.265 |  Val. Acc: 64.34%\n",
      "Min val loss 0.265\n",
      "\n",
      "[Epoch 24, Step   200] loss: 0.246\n",
      "[Epoch 24, Step   400] loss: 0.252\n",
      "[Epoch 24, Step   600] loss: 0.254\n",
      "[24] \tTrain Loss: 0.250 | Train Acc: 65.90%\n",
      "[24] \t Val. Loss: 0.266 |  Val. Acc: 64.32%\n",
      "no improvement = 1\n",
      "\n",
      "[Epoch 25, Step   200] loss: 0.247\n",
      "[Epoch 25, Step   400] loss: 0.239\n",
      "[Epoch 25, Step   600] loss: 0.247\n",
      "[25] \tTrain Loss: 0.245 | Train Acc: 66.44%\n",
      "[25] \t Val. Loss: 0.262 |  Val. Acc: 64.21%\n",
      "Min val loss 0.262\n",
      "\n",
      "[Epoch 26, Step   200] loss: 0.240\n",
      "[Epoch 26, Step   400] loss: 0.245\n",
      "[Epoch 26, Step   600] loss: 0.246\n",
      "[26] \tTrain Loss: 0.243 | Train Acc: 66.50%\n",
      "[26] \t Val. Loss: 0.269 |  Val. Acc: 63.79%\n",
      "no improvement = 1\n",
      "\n",
      "[Epoch 27, Step   200] loss: 0.243\n",
      "[Epoch 27, Step   400] loss: 0.239\n",
      "[Epoch 27, Step   600] loss: 0.242\n",
      "[27] \tTrain Loss: 0.241 | Train Acc: 66.71%\n",
      "[27] \t Val. Loss: 0.267 |  Val. Acc: 64.18%\n",
      "no improvement = 2\n",
      "\n",
      "[Epoch 28, Step   200] loss: 0.241\n",
      "[Epoch 28, Step   400] loss: 0.244\n",
      "[Epoch 28, Step   600] loss: 0.240\n",
      "[28] \tTrain Loss: 0.240 | Train Acc: 66.62%\n",
      "[28] \t Val. Loss: 0.268 |  Val. Acc: 63.96%\n",
      "no improvement = 3\n",
      "\n",
      "[Epoch 29, Step   200] loss: 0.228\n",
      "[Epoch 29, Step   400] loss: 0.235\n",
      "[Epoch 29, Step   600] loss: 0.243\n",
      "[29] \tTrain Loss: 0.235 | Train Acc: 67.04%\n",
      "[29] \t Val. Loss: 0.267 |  Val. Acc: 63.78%\n",
      "no improvement = 4\n",
      "\n",
      "[Epoch 30, Step   200] loss: 0.236\n",
      "[Epoch 30, Step   400] loss: 0.230\n",
      "[Epoch 30, Step   600] loss: 0.234\n",
      "[30] \tTrain Loss: 0.234 | Train Acc: 67.07%\n",
      "[30] \t Val. Loss: 0.264 |  Val. Acc: 64.41%\n",
      "no improvement = 5\n",
      "\n",
      "[Epoch 31, Step   200] loss: 0.229\n",
      "[Epoch 31, Step   400] loss: 0.229\n",
      "[Epoch 31, Step   600] loss: 0.234\n",
      "[31] \tTrain Loss: 0.231 | Train Acc: 67.27%\n",
      "[31] \t Val. Loss: 0.268 |  Val. Acc: 64.16%\n",
      "no improvement = 6\n",
      "\n",
      "[Epoch 32, Step   200] loss: 0.238\n",
      "[Epoch 32, Step   400] loss: 0.226\n",
      "[Epoch 32, Step   600] loss: 0.228\n",
      "[32] \tTrain Loss: 0.230 | Train Acc: 67.23%\n",
      "[32] \t Val. Loss: 0.270 |  Val. Acc: 64.03%\n",
      "no improvement = 7\n",
      "\n",
      "[Epoch 33, Step   200] loss: 0.228\n",
      "[Epoch 33, Step   400] loss: 0.227\n",
      "[Epoch 33, Step   600] loss: 0.227\n",
      "[33] \tTrain Loss: 0.229 | Train Acc: 67.39%\n",
      "[33] \t Val. Loss: 0.282 |  Val. Acc: 61.84%\n",
      "no improvement = 8\n",
      "\n",
      "[Epoch 34, Step   200] loss: 0.223\n",
      "[Epoch 34, Step   400] loss: 0.224\n",
      "[Epoch 34, Step   600] loss: 0.225\n",
      "[34] \tTrain Loss: 0.225 | Train Acc: 67.79%\n",
      "[34] \t Val. Loss: 0.269 |  Val. Acc: 63.86%\n",
      "no improvement = 9\n",
      "\n",
      "[Epoch 35, Step   200] loss: 0.219\n",
      "[Epoch 35, Step   400] loss: 0.225\n",
      "[Epoch 35, Step   600] loss: 0.226\n",
      "[35] \tTrain Loss: 0.223 | Train Acc: 67.85%\n",
      "[35] \t Val. Loss: 0.268 |  Val. Acc: 64.39%\n",
      "Early Stopping\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(cur_epoch,n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # Training\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        images, _, labels = data\n",
    "        \n",
    "        image_input = torch.tensor(np.stack(images)).to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        # zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        output = model(image_input)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += binary_accuracy(output, labels)\n",
    "        if i % 200 == 199:  # print every 200 mini-batches\n",
    "            print('[Epoch %d, Step %5d] loss: %.3f' %\n",
    "                  (epoch, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "    train_loss, train_acc = epoch_loss / len(trainloader), epoch_acc / len(trainloader)\n",
    "\n",
    "    trainval_lossacc['train_loss'].append(train_loss)\n",
    "    trainval_lossacc['train_acc'].append(train_acc)\n",
    "\n",
    "    # Evaluate with Test dataset\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, texts, labels = data\n",
    "            image_input = torch.tensor(np.stack(images)).to(device)\n",
    "            \n",
    "            labels = labels.float().to(device)\n",
    "            # Forward \n",
    "            output = model(image_input)\n",
    "            \n",
    "            # Compute the loss using the final output\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += binary_accuracy(output, labels)\n",
    "    \n",
    "    valid_loss, valid_acc = epoch_loss / len(testloader), epoch_acc / len(testloader)\n",
    "\n",
    "    trainval_lossacc['valid_loss'].append(valid_loss)\n",
    "    trainval_lossacc['valid_acc'].append(valid_acc)\n",
    "    # Showing statistics\n",
    "    print(f'[{epoch}] \\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'[{epoch}] \\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    # Early stopping condition\n",
    "    # https://stackoverflow.com/questions/60200088/how-to-make-early-stopping-in-image-classification-pytorch\n",
    "    if valid_loss < min_val_loss:\n",
    "        min_val_loss = valid_loss\n",
    "        epoch_no_improv = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': trainval_lossacc,\n",
    "            }, MODEL_PATH)\n",
    "        \n",
    "        print(f'Min val loss {min_val_loss:.3f}')\n",
    "    else:\n",
    "        epoch_no_improv += 1\n",
    "        if epoch_no_improv >= patience:\n",
    "            print('Early Stopping')\n",
    "            break\n",
    "            # os.makedirs(f'drive/Shareddrives/MultimodalNews/models/{MODEL_VERSION}/', exist_ok=True)\n",
    "            # torch.save(best_model, f\"drive/Shareddrives/MultimodalNews/models/{MODEL_VERSION}/clipclassifier.pth\")\n",
    "        print(f\"no improvement = {epoch_no_improv}\")\n",
    "    print()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from checkpoint if it exists\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "cur_epoch = checkpoint['epoch']\n",
    "trainval_lossacc = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train loss: 0.24470\n",
      "final train acc: 0.66440\n",
      "\n",
      "final valid loss: 0.26212\n",
      "final valid acc: 0.64212\n"
     ]
    }
   ],
   "source": [
    "print(f\"final train loss: {trainval_lossacc['train_loss'][-1]:.5f}\")\n",
    "print(f\"final train acc: {trainval_lossacc['train_acc'][-1]:.5f}\")\n",
    "print()\n",
    "print(f\"final valid loss: {trainval_lossacc['valid_loss'][-1]:.5f}\")\n",
    "print(f\"final valid acc: {trainval_lossacc['valid_acc'][-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25] \tTrain Loss: 0.24470 | Train Acc: 66.43988%\n",
      "[25] \t Val. Loss: 0.26212 |  Val. Acc: 64.21204%\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{cur_epoch}] \\tTrain Loss: {trainval_lossacc['train_loss'][-1]:.5f} | Train Acc: {trainval_lossacc['train_acc'][-1]*100:.5f}%\")\n",
    "print(f\"[{cur_epoch}] \\t Val. Loss: {trainval_lossacc['valid_loss'][-1]:.5f} |  Val. Acc: {trainval_lossacc['valid_acc'][-1]*100:.5f}%\")\n",
    "# [25] \tTrain Loss: 0.24470 | Train Acc: 66.43988%\n",
    "# [25] \t Val. Loss: 0.26212 |  Val. Acc: 64.21204%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multinews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12a1584764fc80bec723ae766c7274bc1045a0de66e94735d00d14d294f6d446"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
